---
layout: post
title: "Optimize for Understanding"
categories:
- programming
- XS
- Perl
- Djikstra
- life
---

There have been a few times in my short programming career that a principle just "clicked", and they feel less like a tiny click and more like a tsunami raging through my brain. During my final college internship, I learned the importance of automated tests when I turned in a broken program that required another two weeks work just to make correct. Like many others, I learned the general importance of writing maintainable code when I tried to go back and use school projects from the year before. I was never really a speed-demon myself, but this is my experience learning the wisdom in Djikstra's famous "premature optimization is the root of all evil".

A pet project I've had for a long time is [Algorithm::AM], the reference implementation for Analogical Modeling that I took over a few years ago. The original reason I took it over was because I wanted to fix it. I had taken the Analogical Modeling class from its founder at BYU, and during the course I had to borrow computers to run experiments because the program would not run on 64-bit Windows. That really bugged me. Why did it work everywhere but 64-bit Windows?

So I took over the distribution and tinkered for a bit, thinking it might just be a wrong size integer type or something. I'm no C programmer, and the core of the module is written in XS. It turned out to be entirely opaque, and instead of trying to fix it by random edits, I decided to add tests and refactor the the whole thing until I found the bug. This was my first time inheriting and fixing a legacy system, and the process took me many hours over the course of about [a year]. Along the way, I made my own reference implementation in Java, with the goal of understanding the core algorithm, the XS code for which I have almost never touched.

Why did it take so long? The implementation was exactly one Perl package and one XS file, but it was far too complex for me to understand what was going on. The Perl code was almost entirely stored in a giant `DATA` block, with little comments like `### process xyz`. At run time, the options provided by the user would specify whether process `xyz` needed to be done, and if not then this code would be edited out. The edited code was loaded via `eval` and then executed. The author's reason for editing the code ([]hundred lines) at runtime was that it would save some processing time. I, however, abhorred it because 1) it made syntax highlighting difficult and I couldn't tell if the program would compile or not until running it and 2) it made debugging impossible because all of the important code was in a string. On top of that, though, it really did not save much time. The real processing bottleneck in the Perl code was in IO; all of the processing results were printed in copious reports to standard out, and this took FOREVER for big data sets. I remember a grad student waiting two weeks for his results to finish printing (this was 2011, and computers were not that slow). So I began my refactoring quest by writing tests that inspected output, and from there refactored and redesigned until it wasn't too hard to understand how it worked. 

But the interesting part was in the XS code. XS was used for the core algorithm because it is known to be exponential in input size, and we need all the speed we can get to do even puny problems (20-30 variables is the limit). The XS implementation was designed to be parallelizable (if your compiler is smart). It uses custom big integers made from long arrays, and the lattice-filling algorithm is pure genius in my opinion. The author managed to reduce the exponential space requirement to something much closer to linear. The XS, too, was pretty difficult to understand; the author did not like using functions because function calls are "inefficient", so the big integer arithmetic was done directly on the long array pointers inline, and instead of looping `(int i = 0; i < 4; i++)`, he would simply copy and paste the loop four times! After lots of inspection, commenting, and bothering the author with questions, I reduced some of the big integer computations down to macros and found the original issue I had been looking for since years before to be an off-by-one error. I also simplified the code considerably.

Fast-forward another year, and I am working on my Java implementation. The code is much more flexible and iterable (to my mind), and I can experiment with different core algorithms. I can't remove the exponential time requirement, but I improve it so that the Java implementation runs 10x faster than the original C/XS.

Once again, "premature" is the key to Djikstra's statement. The bug in the "super fast" XS code remained for 10 years because nobody dared to touch it, and couldn't find it even if they tried. The Java code, designed for ease of understanding, was able to be optimized later to be far faster than the XS. We should be optimizing our code for understanding now, and worrying about speed later when we can profile and identify bottlenecks.

`local` vars for handlers